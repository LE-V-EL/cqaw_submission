{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e260cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import csv\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f3f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEV1_GT_FILE = 'ADMIN_metadata.csv'\n",
    "LEV2_GT_FILE = '2ADMIN_metadata.csv'\n",
    "LEV3_GT_FILE = '3ADMIN_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34a5df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(filename):\n",
    "    labels = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        csvr = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        for r in csvr:\n",
    "\n",
    "            if r[-1] == 'label' or r[-1] == 'pred_label' or r[-1] == 'predict':\n",
    "                # ignore header\n",
    "                continue\n",
    "                \n",
    "            labels[r[0]] = r\n",
    "\n",
    "    return labels\n",
    "\n",
    "def compare(gt_value, pred_value):\n",
    "    \n",
    "    is_array = False\n",
    "    \n",
    "    if gt_value.startswith('['):\n",
    "        # parse array\n",
    "        is_array = True\n",
    "        gt_value = [float(v) for v in gt_value[1:-1].split(',')]\n",
    "    else:\n",
    "        gt_value = [float(gt_value)]\n",
    "        \n",
    "    if pred_value.startswith('['):\n",
    "        \n",
    "        if not is_array:\n",
    "            # penalize\n",
    "            return 1000000\n",
    "        \n",
    "        # parse array\n",
    "        try:\n",
    "            pred_value = [float(v) for v in pred_value[1:-1].split(',')]\n",
    "        except:\n",
    "            pred_value = []\n",
    "        \n",
    "        if len(gt_value) != len(pred_value):\n",
    "            return 1000000\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if is_array:\n",
    "            # penalize\n",
    "            return 1000000\n",
    "        \n",
    "        pred_value = [float(pred_value)]\n",
    "        \n",
    "#     try:\n",
    "    rmse = mean_squared_error(gt_value, pred_value, squared=False)\n",
    "#     except:\n",
    "#         print(gt_value, pred_value)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def compare3(gt_value, pred_value):\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    if pred_value == '':\n",
    "        return 1000000\n",
    "    \n",
    "    if gt_value == 'Yes' or gt_value == 'No':\n",
    "        if gt_value != pred_value:\n",
    "            error += 1\n",
    "    \n",
    "    elif is_float(gt_value):\n",
    "\n",
    "        error += mean_squared_error([float(gt_value)], [float(pred_value)], squared=False)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Levenstein edit distance\n",
    "        error += nltk.edit_distance(gt_value, pred_value)\n",
    "        \n",
    "    return error\n",
    "\n",
    "def compare_many(gt_values, pred_values, lev3=False):\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for i,v in enumerate(gt_values.keys()):\n",
    "        \n",
    "        if not v in pred_values:\n",
    "            error = 1000000\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if lev3:\n",
    "                error = compare3(gt_values[v][-1], pred_values[v][-1])\n",
    "            else:\n",
    "                error = compare(gt_values[v][-1], pred_values[v][-1])\n",
    "\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bec4621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev1_gt = read_labels(LEV1_GT_FILE)\n",
    "lev2_gt = read_labels(LEV2_GT_FILE)\n",
    "lev3_gt = read_labels(LEV3_GT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "746a8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEV1_SUB_FILE = 'lev1_TEST_metadata.csv'\n",
    "LEV2_SUB_FILE = 'lev2_TEST_metadata.csv'\n",
    "LEV3_SUB_FILE = 'lev3_TEST_metadata.csv'\n",
    "SUBMISSIONS_DIR = '/home/d/Projects/cqaw_submission/SUBMISSIONS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e1108c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SUBMISSION BY SNEAKRZ\n",
      "LEVEL 1 mean RMSE: 29.460651101441833\n",
      "LEVEL 2 mean RMSE: 12943.056222271998\n",
      "LEVEL 3 mean ERROR: 1.3067628375363\n",
      "--------------------------------------------------------------------------------\n",
      "SUBMISSION BY sudoku\n",
      "LEVEL 1 mean RMSE: 39.80684086530146\n",
      "LEVEL 2 mean RMSE: 2376.610556288988\n",
      "LEVEL 3 mean ERROR: 236782.620882325\n",
      "--------------------------------------------------------------------------------\n",
      "SUBMISSION BY ZSY\n",
      "LEVEL 1 mean RMSE: 2.178483077297918\n",
      "LEVEL 2 mean RMSE: 114575.3238453669\n",
      "--------------------------------------------------------------------------------\n",
      "SUBMISSION BY SRK\n",
      "LEVEL 1 mean RMSE: 0.6701709473346102\n",
      "LEVEL 2 mean RMSE: 50.00059038095959\n",
      "LEVEL 3 mean ERROR: 0.008474890499999999\n",
      "--------------------------------------------------------------------------------\n",
      "SUBMISSION BY TEAM_York\n",
      "LEVEL 2 mean RMSE: 26351.605880857434\n",
      "LEVEL 3 mean ERROR: 1652.9160890953815\n"
     ]
    }
   ],
   "source": [
    "submissions = os.listdir(SUBMISSIONS_DIR)\n",
    "for s in submissions:\n",
    "    \n",
    "    print('-'*80)\n",
    "    print('SUBMISSION BY', s)\n",
    "    \n",
    "    lev1_sub_file = os.path.join(SUBMISSIONS_DIR, s, LEV1_SUB_FILE)\n",
    "    lev2_sub_file = os.path.join(SUBMISSIONS_DIR, s, LEV2_SUB_FILE)\n",
    "    lev3_sub_file = os.path.join(SUBMISSIONS_DIR, s, LEV3_SUB_FILE)\n",
    "\n",
    "    if os.path.exists(lev1_sub_file):\n",
    "        lev1_sub_labels = read_labels(lev1_sub_file)\n",
    "        mean_rmse = compare_many(lev1_gt, lev1_sub_labels)\n",
    "        print('LEVEL 1 mean RMSE:', mean_rmse)\n",
    "        \n",
    "    if os.path.exists(lev2_sub_file):\n",
    "        lev2_sub_labels = read_labels(lev2_sub_file)\n",
    "        mean_rmse = compare_many(lev2_gt, lev2_sub_labels)\n",
    "        print('LEVEL 2 mean RMSE:', mean_rmse)\n",
    "        \n",
    "    if os.path.exists(lev3_sub_file):\n",
    "        lev3_sub_labels = read_labels(lev3_sub_file)\n",
    "        mean_err = compare_many(lev3_gt, lev3_sub_labels, lev3=True)\n",
    "        print('LEVEL 3 mean ERROR:', mean_err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971dce50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
